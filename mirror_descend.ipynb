{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Целевая функция\n",
    "\n",
    "Нижний индекс - номер числка/вектора в наборе.\n",
    "\n",
    "$$ x, s, a_i \\in \\mathbb{R}_+^n;\\quad c \\in \\mathbb{R}_+^m; \\quad i=\\overline{1,m} $$\n",
    "\n",
    "$$ f(x) = s^T x - \\sum_{i=1}^m c_i \\log(a_i^T x) + h(x) $$\n",
    "\n",
    "$$ \\nabla f(x) = s + \\sum_{i=1}^m \\dfrac{c_i}{a_i^T x}a_i + \\nabla h(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x, s, c, a, h):\n",
    "    # h - penalty function\n",
    "    # s,c,a - const (see problem statement)\n",
    "    return np.dot(s, x) + sum(c[:] * np.log(np.dot(a[:][:], x))) + h(x)\n",
    "\n",
    "def grad_f(x, s, c, a, grad_h):\n",
    "    # grad_h - penalty function gradient\n",
    "    # s,c,a - const (see problem statement)\n",
    "    return s + sum(c[:] / np.dot(a[:], x) * a[:]) + grad_h(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "$$ \\min_{x\\in\\mathbb{R}^n_+} s^tx - \\sum_{i=1}^m  c_i \\log(a_i^T x) + h(x) $$\n",
    "\n",
    "Или, переформулируя как задачу о седловой точке, \n",
    "\n",
    "$$ \\min_{x\\in\\mathbb{R}^n_+} \\max_{y\\in\\mathbb{R}^m_{++}}  s^tx - y^TAx + \\sum_{i=1}^m  c_i \\log(y_i) + h(x) + c_0 $$\n",
    "\n",
    "где $A = [a_1^T, \\ldots, a_m^T]$ и $c_0 = \\sum_{i=1}^m c_i\\left( 1 - \\log c_i \\right)$ - константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n",
    "В общем виде алгоритм градиентного спуска (верхний индекс - номер итерации):\n",
    "\n",
    "$$ x^{k+1} = x^{k} - \\gamma^{k}\\nabla f(x^k) $$\n",
    "\n",
    "При это алгоритм вычисления значения фунции и градиента функции в точке известен и имеет трудоёмкость $O(m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescend:\n",
    "    \n",
    "    dim   = 0        # dimension\n",
    "    grad  = None     # objective function gradient\n",
    "    alpha = None     # MD coef\n",
    "    coef_func = None # gamma^k\n",
    "\n",
    "    def default_coef_func(it_num, x, grad_val):\n",
    "        return 0.01\n",
    "    \n",
    "    def __init__(self, _dim, _grad, _alpha):\n",
    "        self.dim   = _dim\n",
    "        self.alpha = _alpha\n",
    "        self.grad  = _grad\n",
    "    \n",
    "    def do_nothing(x, t):\n",
    "        return\n",
    "    \n",
    "    def run(self, x0, steps_num = 1000, user_action = do_nothing):\n",
    "        x      = np.zeros(self.dim)\n",
    "        next_x = np.zeros(self.dim)\n",
    "        \n",
    "        x = x0\n",
    "        user_action(x, 0)\n",
    "\n",
    "        for i in range(1, steps_num):\n",
    "            next_x = self.iteration(i, x)\n",
    "            user_action(next_x, i)\n",
    "            x, next_x = next_x, x # swap array references\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def iteration(self, iter_num, x):\n",
    "            return x - np.dot(self.alpha(iter_num, x, self.grad(x)), self.grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD : y* = 2.2499999999999893  y = 2.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81586945, 2.04677104, 3.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_test      = lambda x : 0.0\n",
    "grad_h_test = lambda x : np.zeros(len(x))\n",
    "alpha_test  = lambda k, x, grad_f: 0.001\n",
    "\n",
    "df = lambda x: 4 * x**3 - 9 * x**2 # f = x^4 - 3x^3\n",
    "\n",
    "x0 = [1.0, 2.0, 3.0]\n",
    "s  = [1.0, 0.0, 0.0]\n",
    "c  = [1.0, -1.0, 0.0]\n",
    "a  = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\n",
    "\n",
    "md = GradientDescend(len(x0), df, alpha_test)\n",
    "print \"GD : y* =\", md.run(1.0, 10000), \" y =\", 2.25\n",
    "\n",
    "grad_test = lambda x: grad_f(x, s, c, a, grad_h_test)\n",
    "\n",
    "md = GradientDescend(len(x0), grad_test, alpha_test)\n",
    "md.run(x0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Зеркальный спуск\n",
    "\n",
    "В общем виде алгоритм зеркального спуска (верхний индекс - номер итерации):\n",
    "\n",
    "$$ x^{k+1} = \\arg \\min_{x \\in \\mathbb{R}_+^n} ~ \\left\\langle \\alpha^k \\nabla f(x^k), x \\right\\rangle + V_{x^k}(x) $$\n",
    "\n",
    "Где градиент исследуемой функции $f(x)$:\n",
    "\n",
    "$$ \\nabla f(x^k) = \\alpha^k\\left(s + \\sum_{i=1}^m \\dfrac{c_i}{a_i^T x^k}a_i + \\nabla h(x^k)\\right)$$\n",
    "\n",
    "В нашей задачи для оценки сходимости используется первая норма, поэтому в качестве расстояния Брэгмана мы выберем \n",
    "\n",
    "$$ V_{x}(y) = \\sum_{i=1}^n x_i \\log \\dfrac{y_i}{x_i}$$\n",
    "\n",
    "Итого, переходя к нормированным переменным $\\tilde{x}\\in\\Delta_n$ (после переходя со звёздочкой волну над $x$ опустим):\n",
    "\n",
    "$$ x^{k+1} = \\arg \\min_{x \\in \\mathbb{R}_+^n}  \\left\\langle \\alpha^k \\nabla f(x^k), x \\right\\rangle + \\sum_{i=1}^n x^k_i \\log \\dfrac{x_i}{x^k_i} =^* $$\n",
    "\n",
    "$$ =^*  \\arg \\min_{x \\in \\Delta_n} \\left\\langle \\alpha^k \\nabla f(x^k), x \\right\\rangle + \\sum_{i=1}^n x^k_i \\log \\dfrac{x_i}{x^k_i} = $$\n",
    "\n",
    "$$ = x^k \\cdot \\dfrac{\\exp\\left(-\\alpha^k \\nabla f(x^k) \\right)}{\\left\\lVert x^k\\cdot \\exp\\left(-\\alpha^k \\nabla f(x^k)\\right) \\right\\rVert_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MirrorDescend:\n",
    "    \n",
    "    dim   = 0     # dimension\n",
    "    grad  = None  # objective function gradient\n",
    "    alpha = None  # MD coef\n",
    "\n",
    "    def __init__(self, _dim, _grad, _alpha):\n",
    "        self.dim   = _dim\n",
    "        self.alpha = _alpha\n",
    "        self.grad  = _grad\n",
    "    \n",
    "    def do_nothing(x, t):\n",
    "        return\n",
    "    \n",
    "    def run(self, x0, steps_num = 1000, user_action = do_nothing):\n",
    "        x      = np.zeros(self.dim)\n",
    "        next_x = np.zeros(self.dim)\n",
    "        \n",
    "        x = x0\n",
    "        user_action(x, 0)\n",
    "\n",
    "        for i in range(1, steps_num):\n",
    "            next_x = self.iteration(i, x)\n",
    "            user_action(next_x, i)\n",
    "            x, next_x = next_x, x # swap array references\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def iteration(self, iter_num, x):\n",
    "            z = np.exp(- self.alpha(iter_num) * self.grad(x))\n",
    "            return x * z / np.linalg.norm(x * z, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08914119, 0.42557429, 0.48528452])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_test      = lambda x : 0.0\n",
    "grad_h_test = lambda x : np.zeros(len(x))\n",
    "alpha_test  = lambda k : 0.001\n",
    "\n",
    "x0 = [1.0, 2.0, 3.0]\n",
    "s  = [1.0, 0.0, 0.0]\n",
    "c  = [1.0, -1.0, 0.0]\n",
    "a  = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\n",
    "\n",
    "grad_test   = lambda x : grad_f(x, s, c, a, grad_h_test)\n",
    "\n",
    "md = MirrorDescend(len(x0), grad_test, alpha_test)\n",
    "md.run(x0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite mirror prox algorithm\n",
    "\n",
    "Рассмотрим задачу задачу\n",
    "\n",
    "$$ \\min_{u_1\\in U_1} \\max_{u_1\\in U_1} \\phi(u_1,u_2) + \\Psi_1(u_1) - \\Psi_2(u_2)$$\n",
    "\n",
    "Для неё известен алгоритм \"Composite Mirror Prox\".\n",
    "\n",
    "Pаметим теперь, что данная задача является эквивалентной нашей. Действительно, \n",
    "\n",
    "$$ \\min_{u_1\\in U_1} \\max_{u_1\\in U_1} \\phi(u_1,u_2) + \\Psi_1(u_1) - \\Psi_2(u_2) ~=~ \\min_{x\\in\\mathbb{R}^n_+} \\max_{y\\in\\mathbb{R}^m_{++}}  s^Tx - y^TAx + \\sum_{i=1}^m  c_i \\log(y_i) + h(x) + c_0 $$\n",
    "\n",
    "$$u_1 = x, ~ U_1 = \\mathbb{R}^n_+\\\\ u_2 = y, ~U_2 = \\mathbb{R}^m_{++}$$\n",
    "\n",
    "$$\\phi(x,y) + \\Psi_1(x) - \\Psi_2(y) ~=~ s^Tx - y^TAx + \\sum_{i=1}^m  c_i \\log(y_i) + h(x) + c_0 $$\n",
    "\n",
    "$$ \\phi(x,y) =  s^tx - y^TAx + c_0$$\n",
    "$$ \\Psi_1(x) = h(x);  \\quad \\Psi_2(y) = - \\sum_{i=1}^m  c_i \\log(y_i)  $$\n",
    "\n",
    "Таким образом алгоритм может быть применён для решения поставленной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "phi      = lambda x, y, s, A, c : np.dot(s, x) - np.dot(np.dot(y, A), x) + c\n",
    "grad_phi = lambda x, y, s, A, c : s - np.dot(A, y)\n",
    "\n",
    "psi_1 = lambda x, h : h(x)\n",
    "psi_2 = lambda y, c : sum(c[:] * np.log(np.dot(a[:][:], y)))\n",
    "\n",
    "\n",
    "class CompositeMirrorProx:\n",
    "    \n",
    "    dim    = 0     # dimension\n",
    "    func   = None\n",
    "    grad_1 = None\n",
    "    grad_2 = None\n",
    "    psi_1  = None\n",
    "    psi_2  = None\n",
    "    breg1  = None\n",
    "    breg2  = None\n",
    "    alpha1 = None\n",
    "    alpha2 = None\n",
    "\n",
    "    gamma = None  # MD coef\n",
    "\n",
    "    def __init__(self, _dim, _func, _grad_1, _grad_2, _psi1, _psi2, \n",
    "                 _alpha1, _alpha_2, _breg1, _breg2, _gamma):\n",
    "        self.dim    = _dim\n",
    "        self.func   = _func\n",
    "        self.grad_1   = _grad_1\n",
    "        self.grad_2   = _grad_2\n",
    "        self.psi_1  = _psi1\n",
    "        self.psi_2  = _psi2\n",
    "        self.breg1  = _breg1\n",
    "        self.breg2  = _breg2\n",
    "        self.alpha1 = _alpha1\n",
    "        self.alpha2 = _alpha2\n",
    "        self.gamma  = _gamma\n",
    "\n",
    "    def do_nothing(x, t):\n",
    "        return\n",
    "    \n",
    "    def run(self, u0_1, u0_2, steps_num = 1000, user_action = do_nothing):\n",
    "        u      = np.zeros(self.dim)\n",
    "        next_u = np.zeros(self.dim)\n",
    "        \n",
    "        scipy.optimize.LinearConstraint \n",
    "        \n",
    "        u_1 = u0_1\n",
    "        u_2 = u0_2\n",
    "        user_action([u_1, u_2], 0)    \n",
    "        \n",
    "        for i in range(1, steps_num):\n",
    "            next_u_1, next_u_2 = self.iteration(t, t, u0_1, u0_2, u_1, u_2)\n",
    "            user_action([next_u_1, next_u_2], t)\n",
    "            u_1, next_u_1 = next_u_1, u_1 # swap array references\n",
    "            u_2, next_u_2 = next_u_2, u_2 # swap array references\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def opt_func_1(t, u_1, u_t_1, u_cap1, u_cap2):\n",
    "        return self.alpha1 * self.Breg1(u_1, u_t_1) + \\\n",
    "               np.dot(self.gamma(t) * self.grad_1(u_cap1, u_cap2), u_1) + \\\n",
    "               self.gamma(t) * self.psi_1(u_1)\n",
    "\n",
    "    def opt_func_2(t, u_2, u_t_2, u_cap1, u_cap2):\n",
    "        return self.alpha2 * self.Breg2(u_2, u_t_2) + \\\n",
    "               np.dot(self.gamma(t) * self.grad_2(u_cap1, u_cap2), u_2) + \\\n",
    "               self.gamma(t) * self.psi_2(u_2)\n",
    "    \n",
    "    def opt(func, t, u, u_t, u_cap1, u_cap2):\n",
    "        return minimize(func(t, u, u_t, u_cap1, u_cap2), method='nelder-mead', bounds=(0,np.inf))\n",
    "\n",
    "    def iteration(self, t, u0_1, u0_2, u_1, u_2):       \n",
    "            u_cap_1   = self.opt(self.opt_func_1, t, u0_1, u_1, u_1, u_2)\n",
    "            u_cap_2   = self.opt(self.opt_func_2, t, u0_2, u_2, u_1, u_2)\n",
    "            next_u_1  = self.opt(self.opt_func_1, t, u0_1, u_1, u_cap_1, u_cap_2)\n",
    "            next_u_2  = self.opt(self.opt_func_2, t, u0_2, u_2, u_cap_1, u_cap_2)\n",
    "            return next_u_1, next_u_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
